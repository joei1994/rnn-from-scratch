{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#nltk.download('book')\n",
    "\n",
    "vocabulary_size = 8000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "start_token = \"START_TOKEN\"\n",
    "end_token = \"END_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split comments into tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 79170 sentences in this corpus\n",
      "\"START_TOKEN i joined a new league this year and they have different scoring rules than i'm used to. END_TOKEN\"\n",
      "['START_TOKEN', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'END_TOKEN']\n"
     ]
    }
   ],
   "source": [
    "comments = pd.read_csv(\"data/reddit-comments-2015-08.csv\")\n",
    "\n",
    "#tokenize comment into sentences\n",
    "sentences = [nltk.sent_tokenize(comment.lower()) for comment in comments['body'].values]\n",
    "sentences = list(itertools.chain(*sentences))\n",
    "\n",
    "print(f\"There are total {len(sentences)} sentences in this corpus\")\n",
    "\n",
    "#add start_token and end_token to each sentences\n",
    "sentences = [f\"{start_token} {s} {end_token}\" for s in sentences]\n",
    "print(f\"\\\"{sentences[0]}\\\"\")\n",
    "\n",
    "#split each sentences into word\n",
    "tokenized_sentences = [nltk.word_tokenize(s) for s in sentences]\n",
    "print(tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words is 65408\n",
      "Using vocabulary size of 8000\n",
      "The least frequent word is \"documentary\" which appeared 10 times\n"
     ]
    }
   ],
   "source": [
    "#Word distribution\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "\n",
    "#Number of unique word in corpus\n",
    "print(f\"Number of unique words is {len(word_freq.items())}\")\n",
    "\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "\n",
    "print(f\"Using vocabulary size of {vocabulary_size}\")\n",
    "print(f\"The least frequent word is \\\"{vocab[-1][0]}\\\" which appeared {vocab[-1][1]} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace all words not in vocabulary with UNKNOWN_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = [word_freq[0] for word_freq in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in index_to_word else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X train & Y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray([[word_to_index[word] for word in sent[:-1]] for sent in tokenized_sentences])\n",
    "Y_train = np.asarray([[word_to_index[word] for word in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : [0, 11, 17, 7, 3094, 5974, 7999, 7999, 5974, 2]\n",
      "x : ['START_TOKEN', 'it', \"'s\", 'a', 'slight', 'ppr', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'ppr', '.']\n",
      "y : [11, 17, 7, 3094, 5974, 7999, 7999, 5974, 2, 1]\n",
      "y : ['it', \"'s\", 'a', 'slight', 'ppr', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'ppr', '.', 'END_TOKEN']\n"
     ]
    }
   ],
   "source": [
    "print(f\"x : {X_train[1]}\")\n",
    "print(f\"x : {[index_to_word[index] for index in X_train[1]]}\")\n",
    "print(f\"y : {Y_train[1]}\")\n",
    "print(f\"y : {[index_to_word[index] for index in Y_train[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RNN class using pure Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "Let's recap the equations for the RNN from the first part of the tutorial:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "s_t &= \\tanh(Ux_t + Ws_{t-1}) \\\\\n",
    "o_t &= \\mathrm{softmax}(Vs_t)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "I always find it useful to write down the dimensions of the matrices and vectors. Let's assume we pick a vocabulary size $C = 8000$ and a hidden layer size $H = 100$. You can think of the hidden layer size as the \"memory\" of our network. Making it bigger allows us to learn more complex patterns, but also results in additional computation. Then we have:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "x_t & \\in \\mathbb{R}^{8000} \\\\\n",
    "o_t & \\in \\mathbb{R}^{8000} \\\\\n",
    "s_t & \\in \\mathbb{R}^{100} \\\\\n",
    "U & \\in \\mathbb{R}^{100 \\times 8000} \\\\\n",
    "V & \\in \\mathbb{R}^{8000 \\times 100} \\\\\n",
    "W & \\in \\mathbb{R}^{100 \\times 100} \\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(zs):\n",
    "    exp_zs = [np.exp(z) for z in zs] \n",
    "    return exp_zs / np.sum(exp_zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pdb import set_trace\n",
    "class RNNNumpy:\n",
    "    def __init__(self, n_inputs, n_hiddens = 100, bptt_truncate=4):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        self.U = np.random.uniform(-np.sqrt(1/n_inputs), np.sqrt(1/n_inputs), (n_hiddens, n_inputs))\n",
    "        self.W = np.random.uniform(-np.sqrt(1/n_hiddens), np.sqrt(1/n_hiddens), (n_hiddens, n_hiddens))\n",
    "        self.V = np.random.uniform(-np.sqrt(1/n_hiddens), np.sqrt(1/n_hiddens), (n_inputs, n_hiddens))\n",
    "        \n",
    "    def forward_propagate(self, sentent):\n",
    "        # x is sentent\n",
    "        \n",
    "        T = len(sentent)\n",
    "        s = np.zeros((T + 1, self.n_hiddens))\n",
    "        o = np.zeros((T, self.n_inputs))\n",
    "        \n",
    "        for t in range(T):\n",
    "            s[t] = np.tanh(self.U[:, sentent[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]    \n",
    "            \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_propagate(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def loss(self, xs, ys):\n",
    "        L = 0\n",
    "        for i in range(len(xs)):\n",
    "            o, s = self.forward_propagate(xs[i])\n",
    "            predict_probs = o[np.arange(len(o)), ys[i]]\n",
    "            L += -1 * np.sum(np.log(predict_probs))\n",
    "        return L\n",
    "    \n",
    "    def total_loss(self, xs, ys):\n",
    "        N = np.sum([len(x) for x in xs])\n",
    "        return self.loss(xs, ys) / N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagate(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print &quot;Backpropagation step t=%d bptt step=%d &quot; % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def update_weights(self, x, y, learning_rate):\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "        \n",
    "    def train_with_gd(self, X, y, learning_rate=0.005, n_epochs=100):\n",
    "        for epoch in range(n_epochs):\n",
    "            if(epoch % 10 == 0):\n",
    "                loss = self.total_loss(X, y)\n",
    "                print(f\"Epoch : {epoch}, loss = {loss}\")\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                self.update_weights(X[i], y[i], learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, loss = 8.987186970698227\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(8000)\n",
    "model.train_with_gd(X_train[:20], Y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNNNumpy' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6fed98f14d48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RNNNumpy' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "model.predict(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
